# Introduction

This tutorial will demo a number of technologies. The requirements for completing each section will be introduced in the section.

This presentation was given at LinuxCon 2016 in Toronto. There was a live tutorial given and the video is not yet available. However, there are [slides](https://speakerdeck.com/philips/2016-LinuxCon-NA-CoreOS-A-Tutorial-on-Hyperscale-Infrastructure).

## etcd Basics


**Pre-requisites**

- [etcd and etcdctl](https://github.com/coreos/etcd/releases/tag/v3.0.6) for your platform

First, run `etcd` in a terminal window.

```
./etcd
...
```

Storing and retrieving values is done simply using the put/get subcommands.

```
export ETCDCTL_API=3
./etcdctl put foo bar
```

```
./etcdctl get foo
```

With the `-w` flag additional information can be found. Notice the "revision"? With etcd all keys are revisioned and you can use this revision number to get old values of keys, setup multi-key transactions, and view all changes since a certain time.

Using two differnt put calls we will create a couple of known revisions for the key `foo`.

```
./etcdctl put foo bar -w json
{"header":{"cluster_id":17237436991929493444,"member_id":9372538179322589801,"revision":9,"raft_term":2}}
```

```
./etcdctl put foo toronto -w json
{"header":{"cluster_id":17237436991929493444,"member_id":9372538179322589801,"revision":11,"raft_term":2}}
```

With the revision number etcd can "time-travel" and look at old values of `foo`:

```
./etcdctl get foo --rev 9
foo
bar
```

```
./etcdctl get foo --rev 11
foo
toronto
```

```
./etcdctl get foo -w json
{"header":{"cluster_id":17237436991929493444,"member_id":9372538179322589801,"revision":11,"raft_term":2},"kvs":[{"key":"Zm9v","create_revision":2,"mod_revision":11,"version":10,"value":"dG9yb250bw=="}],"count":1}
```


## etcd Clustering


**Pre-requisites**

- A working [Go environment](https://golang.org/doc/install)
- Follow the upstream guide to [setup a local cluster](https://github.com/coreos/etcd/blob/master/Documentation/dev-guide/local_cluster.md#local-multi-member-cluster).

## Building an Application

## Kubernetes Cluster

## Pre-Requisites

- An [AWS account](http://aws.amazon.com/) and [AWS cli](https://aws.amazon.com/cli/)
  - An [AWS keypair for us-west-2](https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2#KeyPairs:sort=keyName)
- [kube-aws](https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html) installed and in your path
- [kubectl 1.3.4+](https://coreos.com/kubernetes/docs/latest/configure-kubectl.html) installed and in your path

## Testing Pre-Requistes

**AWS CLI**

Test that we have a keypair that works and is avaiable in our region:

```
aws ec2 --region us-west-2 describe-key-pairs
{
    "KeyPairs": [
        {
            "KeyName": "philips",
            "KeyFingerprint": "d5:1d:22:c5:cb:57:c3:8d:25:4b:29:f0:f2:9a:96:c9"
        }
    ]
}
```

**kube-aws**

```
kube-aws --help
kube-aws version
```

**kubectl**

```
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"2", GitVersion:"v1.2.3", GitCommit:"882d296a99218da8f6b2a340eb0e81c69e66ecc7", GitTreeState:"clean"}
```

## Initial Cluster Setup

Create a directory for cluster configuration and move into it before following the `kube-aws` setup below.

```
mkdir toronto
cd toronto
```

Follow the [Kubernetes + CoreOS + AWS docs](https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html). At the `kube-aws init` step use toronto.example.com as the `external-dns-name` unless you have a registered domain you wish to use for this cluster and "toronto" as the `cluster-name`. The rest of the tutorial will depend on these two values.

The final step of the AWS instructions, `kube-aws up`, wll take some time. While it is building out that infrastructure we can cover the TLS setup.

### Understanding the Credential Setup

The `credentials` directory has several x509 private keys and certificates and a single certificate authority generated:

```
ls -la credentials/
-rw-------  1 philips  staff  1675 May 13 17:50 admin-key.pem
-rw-------  1 philips  staff  1086 May 13 17:50 admin.pem
-rw-------  1 philips  staff  1675 May 13 17:50 apiserver-key.pem
-rw-------  1 philips  staff  1273 May 13 17:50 apiserver.pem
-rw-------  1 philips  staff  1675 May 13 17:50 ca-key.pem
-rw-------  1 philips  staff  1070 May 13 17:50 ca.pem
-rw-------  1 philips  staff  1679 May 13 17:50 worker-key.pem
-rw-------  1 philips  staff  1155 May 13 17:50 worker.pem
```

The CA generated by `kube-aws` is used to sign three different types of keys:

**api server key**: This is the Kubernetes API server certificate to secure API traffic.
**admin key**: This key authenticates kubectl to the API server. It is referenced in kubeconfg.
**worker**: This key authenticates the worker machines to the API server.

Confirm the admin key is signed by the CA by using the openssl tool:

```
openssl x509 -in credentials/admin.pem -text -noout
```

A production environment might have different sub-CAs but this configuration is a good trade-off of simplicity and security.

### Configure and Test the Cluster

The cluster should be booted now. Confirm by running `kube-aws status`:

```
kube-aws status
Cluster Name:   mycluster
Controller IP:  xx.xx.xx.xx
```

Add an entry in `/etc/hosts` for mycluster.example.com with the IP address printed above 

```
cat /etc/hosts
127.0.0.1	localhost
xx.xx.xx.xx mycluster.example.com
```

Configure kubectl to use the configuration that kube-aws generated:

```
alias kubectl="kubectl --kubeconfig=${PWD}/kubeconfig"
```

And access the cluster to confirm that two nodes have been registered:

```
kubectl get nodes
```

# Launch Our Production Application

With the cluster up launching the application is easy. The guestbook directory has everything needed to launch the app:

```
cd ../guestbook
kubectl create -f guestbook-controller.json
kubectl create -f guestbook-service.json
kubectl create -f redis-master-controller.json
kubectl create -f redis-master-service.json
kubectl create -f redis-slave-controller.json
kubectl create -f redis-slave-service.json
```

This will create a ELB that we can hit. But, it will take a few moments for everything to start, DNS to resolve and for health check to put stuff into the LB. While we are waiting lets dig through the app configurations.

*Read through each JSON file in turn, explaining the layout of the app*.

Now that we have a sense of how everything works try to go to the ELB console page and take a look at the health checks:

https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2#LoadBalancers:

Now that everything is up and running hit the ELB URL in your browser.


```
aws elb describe-load-balancers | jq '"http://" + .LoadBalancerDescriptions[].DNSName' -r
http://a8eedeefe1b4d11e685410a4b212ca4d-2012685803.us-west-2.elb.amazonaws.com
```

# Understand the Network

Port-forward cluster local DNS to your workstation.

```
kubectl port-forward --namespace=kube-system $( kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o template --template="{{range.items}}{{.metadata.name}}{{end}}") 5300:53
```

Try and grab the redis-master service powering our website:

```
dig +vc -p 5300 @127.0.0.1  redis-master.default.svc.cluster.local
redis-master.default.svc.cluster.local. 30 IN A 10.3.0.25
```

For more [network debugging tips see this page](https://github.com/coreos/docs/blob/master/kubernetes/network-troubleshooting.md).

